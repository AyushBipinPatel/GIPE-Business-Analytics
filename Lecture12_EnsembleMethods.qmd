---
title: "Business Analytics"
subtitle: "Ensemble Methods"
author: "Ayush Patel and Jayati Sharma"
date: today
date-format: " DD MMMM, YYYY"
embed-resources: true
format: 
  revealjs:
    embed-resources: true
    slide-number: c/t
    width: 1400
    theme: [serif, theme.scss]
---

## Pre-requisite {.scrollable}

::: incremental
You already....

+ Understand linear regression
+ Learnt about decision trees
:::

## About me {.scrollable}

::: columns
::: {.column width="70%"}
I am [Ayush]{.fragment fragment-index="1" style="font-size:45px"}.

[I am a researcher working at the intersection of data, law, development and economics.]{.fragment fragment-index="2" style="font-size:25px"}

[I teach Data Science using R at Gokhale Institute of Politics and Economics]{.fragment fragment-index="3" style="font-size:25px"}

[I am a [RStudio (Posit) certified tidyverse Instructor.](https://education.rstudio.com/trainers/people/patel+ayush/)]{.fragment fragment-index="4" style="font-size:25px"}

[I am a Researcher at [Oxford Poverty and Human development Initiative (OPHI)](https://ophi.org.uk/), at the University of Oxford.]{.fragment fragment-index="5" style="font-size:25px"}
:::

::: {.column width="30%"}
**Reach me**

{{< fa solid envelope >}} [ayush.ap58\@gmail.com]{style="font-size:25px"}

{{< fa solid envelope >}} [ayush.patel\@gipe.ac.in]{style="font-size:25px"}
:::
:::

## Learning Objectives {.scrollable}

::: incremental

+ Learn about ensemble methods
  - Bagging
  - Boosting
  - Random Forests
  
:::

## Ensemble Methods - Why?{.scrollable}
#### [Content for this topic has been sourced from [An Introduction to Statistical Learning](https://www.statlearning.com). Please check out the work for detailed information.]{style="font-size:15px"}

+ In the previous lecture, you learnt about decision trees
+ Decision trees can be non-robust and not highly accurate
+ Hence, you can aggregate many decision trees
+ These methods are bagging, boosting and random forests

## Ensemble Methods - What?{.scrollable}
#### [Content for this topic has been sourced from [An Introduction to Statistical Learning](https://www.statlearning.com). Please check out the work for detailed information.]{style="font-size:15px"}

+ An ensemble method combines many simple “building block” models in order to obtain a single and potentially very powerful model
+ In simple words, instead of using a single model, this approach improves the accuracyin results by combining many models
+ The "simple building block" models are also known as weak learners, since on their own, they may lead to mediocre predictions with less accuracy

## Bagging{.scrollable}
#### [Content for this topic has been sourced from [An Introduction to Statistical Learning](https://www.statlearning.com). Please check out the work for detailed information.]{style="font-size:15px"}

::: incremental
+ Decision trees have ***high variance***
+ Why? Because when you split your data into **training** and **testing** data to fit a decision tree, the results would have high contrast
+ Hence, a model with low variance would will give similar results when applied to distinct data
+ ***Bootstrap aggregation*** or ***bagging*** is an approach to reduce this variance
:::

## Bagging{.scrollable}
#### [Content for this topic has been sourced from [An Introduction to Statistical Learning](https://www.statlearning.com). Please check out the work for detailed information.]{style="font-size:15px"}


+ Bagging involves taking many training sets, building a model using each and then taking an average of the predictions
+ Why? Because **averaging a set of observations reduces variance**
+ So, essentially bagging builds many such models and averages their results to reduce the variance that comes from a single model
+ The average of predictions by separate training models $f^1 (x)$, $f^2 (x)$, ...... $f^B (x)$ is given by

$$\hat f_{avg} (x) = 1/B sum_{b=1}^B \hat f^b (x)$$

## Bagging{.scrollable}
#### [Content for this topic has been sourced from [An Introduction to Statistical Learning](https://www.statlearning.com). Please check out the work for detailed information.]{style="font-size:15px"}

Hold up..

+ We won't always have so many datasets
+ So how would we able to generate so many training sets?
+ Simply put, we take repeated samples from the single training set
+ 'B' regression trees are constructed using 'B' bootstrapped training sets, and we take an average of the resulting predictions

## Bagging{.scrollable}
#### [Content for this topic has been sourced from [An Introduction to Statistical Learning](https://www.statlearning.com). Please check out the work for detailed information.]{style="font-size:15px"}

+ This way, each individual tree has a high variance
+ But averaging reduces the variance
+ Recall the bias between flexibility and interpretability?
+ While bagging improves accuracy, it becomes more complex to interpret than decision trees

## Boosting{.scrollable}
#### [Content for this topic has been sourced from [An Introduction to Statistical Learning](https://www.statlearning.com). Please check out the work for detailed information.]{style="font-size:15px"}